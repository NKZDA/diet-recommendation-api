{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd9ccf97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ‡πÉ‡∏ä‡πâ‡πÑ‡∏ü‡∏•‡πå: C:\\Users\\ACER\\OneDrive\\Desktop\\is3\\black end\\datasetis.csv\n",
      "   ‚Üí ‡∏≠‡πà‡∏≤‡∏ô‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏î‡πâ‡∏ß‡∏¢ sep=None, encoding='utf-8-sig'\n",
      "üéØ ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á Target: ['Balanced' 'Low_Carb' 'Low_Sodium']\n",
      "üìä ‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏Ñ‡∏•‡∏≤‡∏™:\n",
      " Diet_Recommendation\n",
      "Balanced      0.426\n",
      "Low_Sodium    0.316\n",
      "Low_Carb      0.258\n",
      "Name: proportion, dtype: float64\n",
      "üîñ Target mapping: {'Balanced': 0, 'Low_Carb': 1, 'Low_Sodium': 2}\n",
      "üîÑ ‡∏à‡∏∞‡πÉ‡∏ä‡πâ SMOTE ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏ù‡∏∂‡∏Å (‡∏û‡∏ö imbalanced-learn)\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "\n",
      "üèÜ Best CV F1_macro: 1.0000\n",
      "üîß Best params: {'model__n_estimators': 150, 'model__min_samples_split': 5, 'model__min_samples_leaf': 4, 'model__max_features': None, 'model__max_depth': 20}\n",
      "\n",
      "üìà Test Accuracy : 1.0\n",
      "üìä Test F1_macro : 1.0\n",
      "\n",
      "üìã Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Balanced       1.00      1.00      1.00       128\n",
      "    Low_Carb       1.00      1.00      1.00        77\n",
      "  Low_Sodium       1.00      1.00      1.00        95\n",
      "\n",
      "    accuracy                           1.00       300\n",
      "   macro avg       1.00      1.00      1.00       300\n",
      "weighted avg       1.00      1.00      1.00       300\n",
      "\n",
      "üß© Confusion Matrix:\n",
      " [[128   0   0]\n",
      " [  0  77   0]\n",
      " [  0   0  95]]\n",
      "\n",
      "üíæ Saved: diet_recommendation_model.joblib, label_encoder.joblib\n",
      "\n",
      "üß™ Example prediction: Balanced {'Balanced': 1.0, 'Low_Carb': 0.0, 'Low_Sodium': 0.0}\n",
      "\n",
      "[DEBUG] predict_one defined? -> True\n",
      "[DEBUG] inference predict sanity -> [2]\n",
      "[DEBUG] dtypes of X.head(1):\n",
      " Age                                   int64\n",
      "Gender                               object\n",
      "Weight_kg                           float64\n",
      "Height_cm                             int64\n",
      "BMI                                 float64\n",
      "Disease_Type                         object\n",
      "Severity                             object\n",
      "Physical_Activity_Level              object\n",
      "Daily_Caloric_Intake                  int64\n",
      "Cholesterol_mg/dL                   float64\n",
      "Blood_Pressure_mmHg                   int64\n",
      "Glucose_mg/dL                       float64\n",
      "Dietary_Restrictions                 object\n",
      "Allergies                            object\n",
      "Preferred_Cuisine                    object\n",
      "Weekly_Exercise_Hours               float64\n",
      "Adherence_to_Diet_Plan              float64\n",
      "Dietary_Nutrient_Imbalance_Score    float64\n",
      "dtype: object\n",
      "[DEBUG] dtypes of sample before cast:\n",
      " Age                                   int64\n",
      "Gender                               object\n",
      "Weight_kg                           float64\n",
      "Height_cm                             int64\n",
      "BMI                                 float64\n",
      "Disease_Type                         object\n",
      "Severity                             object\n",
      "Physical_Activity_Level              object\n",
      "Daily_Caloric_Intake                  int64\n",
      "Cholesterol_mg/dL                   float64\n",
      "Blood_Pressure_mmHg                   int64\n",
      "Glucose_mg/dL                       float64\n",
      "Dietary_Restrictions                float64\n",
      "Allergies                            object\n",
      "Preferred_Cuisine                    object\n",
      "Weekly_Exercise_Hours               float64\n",
      "Adherence_to_Diet_Plan              float64\n",
      "Dietary_Nutrient_Imbalance_Score    float64\n",
      "dtype: object\n",
      "[DEBUG] keys mismatch? missing: set()  extra: set()\n"
     ]
    }
   ],
   "source": [
    "# ==== diet_model_pipeline_full_final2.py ====\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline as SkPipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 1) ‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå CSV ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ (‡∏õ‡∏£‡∏±‡∏ö ROOT_DIRS ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á)\n",
    "# -------------------------------------------------\n",
    "ROOT_DIRS = [\n",
    "    Path.cwd(),                                  # ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ô‡∏™‡∏Ñ‡∏£‡∏¥‡∏õ‡∏ï‡πå\n",
    "    Path(r\"C:\\Users\\ACER\\OneDrive\\Desktop\\s3\"),  # ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\n",
    "]\n",
    "PATTERNS = [\"*data*set*is*.csv\", \"*dataset*is*.csv\", \"*.csv\"]\n",
    "\n",
    "def find_csv(roots, patterns):\n",
    "    for root in roots:\n",
    "        if not root.exists():\n",
    "            continue\n",
    "        for pat in patterns:\n",
    "            for p in root.glob(pat):\n",
    "                if p.is_file():\n",
    "                    return p.resolve()\n",
    "    return None\n",
    "\n",
    "csv_path = find_csv(ROOT_DIRS, PATTERNS)\n",
    "if not csv_path:\n",
    "    raise FileNotFoundError(\"‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå CSV ‡πÉ‡∏ô ROOT_DIRS ‚Äî ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÅ‡∏Å‡πâ ROOT_DIRS ‡∏´‡∏£‡∏∑‡∏≠‡∏ß‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏ß‡πâ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏™‡∏Ñ‡∏£‡∏¥‡∏õ‡∏ï‡πå\")\n",
    "print(f\"‚úÖ ‡πÉ‡∏ä‡πâ‡πÑ‡∏ü‡∏•‡πå: {csv_path}\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2) ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (fallback ‡∏´‡∏•‡∏≤‡∏¢ encoding/sep)\n",
    "# -------------------------------------------------\n",
    "read_ok, last_err = False, None\n",
    "for sep in [None, \",\", \";\", \"\\t\", \"|\"]:\n",
    "    for enc in [\"utf-8-sig\", \"utf-8\", \"cp874\", \"latin-1\"]:\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path, sep=sep, encoding=enc, engine=\"python\")\n",
    "            read_ok = True\n",
    "            print(f\"   ‚Üí ‡∏≠‡πà‡∏≤‡∏ô‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏î‡πâ‡∏ß‡∏¢ sep={repr(sep)}, encoding='{enc}'\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "    if read_ok:\n",
    "        break\n",
    "if not read_ok:\n",
    "    raise RuntimeError(f\"‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {last_err}\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3) ‡πÅ‡∏¢‡∏Å Target / Features\n",
    "# -------------------------------------------------\n",
    "POSSIBLE_TARGETS = [\"Diet_Recommendation\", \"diet_recommendation\", \"Target\"]\n",
    "target_col = next((c for c in POSSIBLE_TARGETS if c in df.columns), None)\n",
    "if not target_col:\n",
    "    raise KeyError(f\"‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå Target ‡πÉ‡∏ô {list(df.columns)} ‚Äî ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÅ‡∏Å‡πâ POSSIBLE_TARGETS ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏à‡∏£‡∏¥‡∏á\")\n",
    "\n",
    "y = df[target_col]\n",
    "X = df.drop(columns=[target_col])\n",
    "\n",
    "print(\"üéØ ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á Target:\", y.unique()[:5])\n",
    "print(\"üìä ‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏Ñ‡∏•‡∏≤‡∏™:\\n\", y.value_counts(normalize=True).round(3))\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 4) ‡πÅ‡∏ö‡πà‡∏á‡∏ä‡∏ô‡∏¥‡∏î‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå + ‡πÄ‡∏Ç‡πâ‡∏≤‡∏£‡∏´‡∏±‡∏™ Target\n",
    "# -------------------------------------------------\n",
    "categorical_cols = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "numeric_cols     = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "print(\"üîñ Target mapping:\", {cls: int(i) for i, cls in enumerate(le.classes_)})\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 5) Train/Test split\n",
    "# -------------------------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.30, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 6) Preprocessor (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö sklearn ‡πÄ‡∏Å‡πà‡∏≤/‡πÉ‡∏´‡∏°‡πà)\n",
    "# -------------------------------------------------\n",
    "try:\n",
    "    categorical_tf = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)  # >=1.2\n",
    "except TypeError:\n",
    "    categorical_tf = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)         # <1.2\n",
    "\n",
    "numeric_tf = StandardScaler()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", categorical_tf, categorical_cols),\n",
    "        (\"num\", numeric_tf, numeric_cols),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 7) Pipeline ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ù‡∏∂‡∏Å (‡πÉ‡∏ä‡πâ SMOTE ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)\n",
    "# -------------------------------------------------\n",
    "use_smote = False\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "    smote = SMOTE(random_state=42)\n",
    "    use_smote = True\n",
    "    print(\"üîÑ ‡∏à‡∏∞‡πÉ‡∏ä‡πâ SMOTE ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏ù‡∏∂‡∏Å (‡∏û‡∏ö imbalanced-learn)\")\n",
    "except Exception:\n",
    "    smote = None\n",
    "    print(\"‚ÑπÔ∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö imbalanced-learn ‚Üí ‡∏Ç‡πâ‡∏≤‡∏° SMOTE (‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á: pip install imbalanced-learn)\")\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42, class_weight=\"balanced\")\n",
    "if use_smote:\n",
    "    train_pipe = ImbPipeline([(\"prep\", preprocessor), (\"smote\", smote), (\"model\", rf)])\n",
    "else:\n",
    "    train_pipe = SkPipeline([(\"prep\", preprocessor), (\"model\", rf)])\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 8) RandomizedSearch + CV\n",
    "# -------------------------------------------------\n",
    "param_dist = {\n",
    "    \"model__n_estimators\": [150, 250, 400, 600],\n",
    "    \"model__max_depth\": [None, 8, 12, 20],\n",
    "    \"model__min_samples_split\": [2, 5, 10],\n",
    "    \"model__min_samples_leaf\": [1, 2, 4],\n",
    "    \"model__max_features\": [\"sqrt\", \"log2\", None],\n",
    "}\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    estimator=train_pipe,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=12,\n",
    "    scoring=\"f1_macro\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=1,\n",
    ")\n",
    "search.fit(X_train, y_train)\n",
    "print(f\"\\nüèÜ Best CV F1_macro: {search.best_score_:.4f}\")\n",
    "print(\"üîß Best params:\", search.best_params_)\n",
    "\n",
    "best_model = search.best_estimator_\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 9) ‡∏™‡∏£‡πâ‡∏≤‡∏á inference_model ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏à‡∏£‡∏¥‡∏á (‡πÑ‡∏°‡πà‡∏°‡∏µ SMOTE)\n",
    "# -------------------------------------------------\n",
    "if \"smote\" in best_model.named_steps:\n",
    "    prep_fitted  = best_model.named_steps[\"prep\"]\n",
    "    model_fitted = best_model.named_steps[\"model\"]\n",
    "    inference_model = SkPipeline([(\"prep\", prep_fitted), (\"model\", model_fitted)])\n",
    "else:\n",
    "    inference_model = best_model\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 10) ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ö‡∏ô Test set\n",
    "# -------------------------------------------------\n",
    "y_pred = inference_model.predict(X_test)\n",
    "print(\"\\nüìà Test Accuracy :\", round(accuracy_score(y_test, y_pred), 4))\n",
    "print(\"üìä Test F1_macro :\", round(f1_score(y_test, y_pred, average='macro'), 4))\n",
    "print(\"\\nüìã Classification report:\\n\", classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "print(\"üß© Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 11) ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• + LabelEncoder\n",
    "# -------------------------------------------------\n",
    "joblib.dump(inference_model, \"diet_recommendation_model.joblib\")\n",
    "joblib.dump(le, \"label_encoder.joblib\")\n",
    "print(\"\\nüíæ Saved: diet_recommendation_model.joblib, label_encoder.joblib\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 12) ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ 1 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ (‡∏Å‡∏±‡∏ô‡∏û‡∏±‡∏á: ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå/‡∏ä‡∏ô‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏ï‡∏≠‡∏ô‡πÄ‡∏ó‡∏£‡∏ô)\n",
    "# -------------------------------------------------\n",
    "def predict_one(sample_dict: dict):\n",
    "    \"\"\"\n",
    "    ‡∏£‡∏±‡∏ö dict 1 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ (‡∏Ñ‡∏µ‡∏¢‡πå‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå X ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÄ‡∏ó‡∏£‡∏ô)\n",
    "    ‡∏Ñ‡∏∑‡∏ô: (label_text, proba_by_class: dict)\n",
    "    \"\"\"\n",
    "    sample_df = pd.DataFrame([sample_dict])\n",
    "\n",
    "    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö X ‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏¥‡∏°‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏´‡∏≤‡∏¢‡πÑ‡∏õ\n",
    "    sample_df = sample_df.reindex(columns=X.columns)\n",
    "\n",
    "    # ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏ä‡∏ô‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á\n",
    "    for col in numeric_cols:\n",
    "        sample_df[col] = pd.to_numeric(sample_df[col], errors=\"coerce\")\n",
    "    for col in categorical_cols:\n",
    "        # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô NaN ‡πÉ‡∏´‡πâ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô string \"nan\" ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ scaler/encoder ‡∏•‡πâ‡∏°\n",
    "        sample_df[col] = sample_df[col].astype(str)\n",
    "\n",
    "    pred_num   = inference_model.predict(sample_df)[0]\n",
    "    pred_label = le.inverse_transform([pred_num])[0]\n",
    "    proba      = inference_model.predict_proba(sample_df)[0]\n",
    "    return pred_label, dict(zip(le.classes_, map(float, proba)))\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 13) ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô + ‡∏ö‡∏•‡πá‡∏≠‡∏Å DEBUG\n",
    "# -------------------------------------------------\n",
    "ex = X.iloc[0].to_dict()          # ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ X_test.iloc[0].to_dict() ‡∏Å‡πá‡πÑ‡∏î‡πâ\n",
    "label, proba = predict_one(ex)\n",
    "print(\"\\nüß™ Example prediction:\", label, proba)\n",
    "\n",
    "# (‡∏ñ‡πâ‡∏≤‡∏ï‡∏¥‡∏î ‡πÉ‡∏´‡πâ‡∏î‡∏π‡∏ö‡∏•‡πá‡∏≠‡∏Å‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á)\n",
    "print(\"\\n[DEBUG] predict_one defined? ->\", callable(globals().get(\"predict_one\", None)))\n",
    "print(\"[DEBUG] inference predict sanity ->\", inference_model.predict(X_test.head(1)))\n",
    "tmp_before = pd.DataFrame([ex]).reindex(columns=X.columns)\n",
    "print(\"[DEBUG] dtypes of X.head(1):\\n\", X.head(1).dtypes)\n",
    "print(\"[DEBUG] dtypes of sample before cast:\\n\", tmp_before.dtypes)\n",
    "missing = set(X.columns) - set(tmp_before.columns)\n",
    "extra   = set(tmp_before.columns) - set(X.columns)\n",
    "print(\"[DEBUG] keys mismatch? missing:\", missing, \" extra:\", extra)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87f6d459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== XGBoost (‡∏ï‡πà‡∏≠‡∏à‡∏≤‡∏Å Baseline) =====\n",
      "Accuracy: 1.0000\n",
      "F1_macro: 1.0000\n",
      "Kappa: 1.0000\n",
      "Total number of instances (after ignore): 300\n",
      "\n",
      "üìä Detailed Accuracy By Class\n",
      "Class                Precision    Recall  F1-Score   Support\n",
      "------------------------------------------------------------\n",
      "Balanced                 1.000     1.000     1.000       128\n",
      "Low_Carb                 1.000     1.000     1.000        77\n",
      "Low_Sodium               1.000     1.000     1.000        95\n",
      "\n",
      "üß© Confusion Matrix (rows=true, cols=predicted)\n",
      "              Balanced    Low_Carb  Low_Sodium\n",
      "Balanced           128           0           0\n",
      "Low_Carb             0          77           0\n",
      "Low_Sodium           0           0          95\n",
      "\n",
      "‚úÖ Correctly classified instances:   300 / 300  (100.00%)\n",
      "‚ùå Incorrectly classified instances: 0 / 300  (0.00%)\n",
      "\n",
      "üìê Mean absolute error (MAE): 0.002956\n",
      "üìê Root mean squared error (RMSE): 0.002619\n",
      "üìè Relative absolute error (RAE): 0.45%\n",
      "üìè Root relative squared error (RRSE): 0.46%\n"
     ]
    }
   ],
   "source": [
    "# ===== XGBoost + WEKA-style summary (‡∏ï‡πà‡∏≠‡∏à‡∏≤‡∏Å Random Forest baseline) =====\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, confusion_matrix,\n",
    "    precision_recall_fscore_support, cohen_kappa_score\n",
    ")\n",
    "\n",
    "# 0) ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á/‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤ XGBoost\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "except Exception as e:\n",
    "    raise ImportError(\"‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á xgboost ‡∏Å‡πà‡∏≠‡∏ô: pip install xgboost\") from e\n",
    "\n",
    "# 1) ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏à‡∏≤‡∏Å baseline ‡πÅ‡∏•‡πâ‡∏ß (X, y, le, categorical_cols, numeric_cols, preprocessor)\n",
    "#    ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏ï‡πà‡∏≠‡∏ó‡∏±‡∏ô‡∏ó‡∏µ; ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ ‡πÉ‡∏´‡πâ‡πÇ‡∏´‡∏•‡∏î/‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÉ‡∏´‡πâ‡πÅ‡∏ö‡∏ö‡∏¢‡πà‡∏≠:\n",
    "if 'X' not in globals():\n",
    "    import pandas as pd\n",
    "    from pathlib import Path\n",
    "    csv_path = Path(r\"C:\\Users\\ACER\\OneDrive\\Desktop\\s3\\data set is.csv\")  # ‡πÅ‡∏Å‡πâ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á\n",
    "    df = pd.read_csv(csv_path, encoding=\"utf-8-sig\")\n",
    "    target_col = \"Diet_Recommendation\"\n",
    "    y_text = df[target_col]\n",
    "    X = df.drop(columns=[target_col])\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y_text)\n",
    "    categorical_cols = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "    numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    try:\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "    except TypeError:\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "    preprocessor = ColumnTransformer(\n",
    "        [(\"cat\", ohe, categorical_cols), (\"num\", StandardScaler(), numeric_cols)],\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "\n",
    "# 2) ‡πÅ‡∏ö‡πà‡∏á‡∏ä‡∏∏‡∏î (‡∏Ñ‡∏á random_state/stratify ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö baseline ‡πÑ‡∏î‡πâ‡∏¢‡∏∏‡∏ï‡∏¥‡∏ò‡∏£‡∏£‡∏°)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 3) ‡πÇ‡∏°‡πÄ‡∏î‡∏• XGBoost + Pipeline\n",
    "xgb = XGBClassifier(\n",
    "    objective=\"multi:softprob\",\n",
    "    num_class=len(np.unique(y)),\n",
    "    tree_method=\"hist\",\n",
    "    eval_metric=\"mlogloss\",\n",
    "    random_state=42,\n",
    "    n_estimators=400,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    ")\n",
    "xgb_pipe = Pipeline([(\"prep\", preprocessor), (\"model\", xgb)])\n",
    "\n",
    "# 4) ‡πÄ‡∏ó‡∏£‡∏ô\n",
    "xgb_pipe.fit(X_train, y_train)\n",
    "\n",
    "# 5) ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢\n",
    "y_pred = xgb_pipe.predict(X_test)\n",
    "\n",
    "# 6) ===== WEKA-style SUMMARY (‡∏Ñ‡∏£‡∏ö‡∏ó‡∏∏‡∏Å‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠) =====\n",
    "# ‡∏ó‡∏≥ y_true/y_pred ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô Series ‡∏ú‡∏π‡∏Å index ‡∏Å‡∏±‡∏ö X_test ‡πÅ‡∏•‡πâ‡∏ß‡∏Å‡∏£‡∏≠‡∏á unknown classes\n",
    "classes = list(le.classes_)\n",
    "y_true_s = pd.Series(y_test, index=X_test.index)       # ‡πÄ‡∏õ‡πá‡∏ô label-encoded (‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç)\n",
    "y_pred_s = pd.Series(y_pred, index=X_test.index)\n",
    "mask = y_true_s.notna() & y_true_s.isin(range(len(classes)))  # ignore class unknown instances\n",
    "\n",
    "y_true = y_true_s.loc[mask].to_numpy()\n",
    "y_pred = y_pred_s.loc[mask].to_numpy()\n",
    "X_eval = X_test.loc[mask]\n",
    "\n",
    "# ‡πÄ‡∏°‡∏ï‡∏£‡∏¥‡∏Å‡∏´‡∏•‡∏±‡∏Å\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "f1m = f1_score(y_true, y_pred, average=\"macro\")\n",
    "kappa = cohen_kappa_score(y_true, y_pred)\n",
    "print(\"\\n===== XGBoost (‡∏ï‡πà‡∏≠‡∏à‡∏≤‡∏Å Baseline) =====\")\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(f\"F1_macro: {f1m:.4f}\")\n",
    "print(f\"Kappa: {kappa:.4f}\")\n",
    "print(f\"Total number of instances (after ignore): {len(y_true)}\")\n",
    "\n",
    "# Detailed accuracy by class\n",
    "prec, rec, f1, sup = precision_recall_fscore_support(\n",
    "    y_true, y_pred, labels=range(len(classes)), zero_division=0\n",
    ")\n",
    "print(\"\\nüìä Detailed Accuracy By Class\")\n",
    "print(f\"{'Class':<20}{'Precision':>10}{'Recall':>10}{'F1-Score':>10}{'Support':>10}\")\n",
    "print(\"-\"*60)\n",
    "for i, cls in enumerate(classes):\n",
    "    print(f\"{cls:<20}{prec[i]:>10.3f}{rec[i]:>10.3f}{f1[i]:>10.3f}{int(sup[i]):>10}\")\n",
    "\n",
    "# Confusion matrix (‡πÄ‡∏•‡∏Ç‡∏î‡∏¥‡∏ö)\n",
    "cm = confusion_matrix(y_true, y_pred, labels=range(len(classes)))\n",
    "print(\"\\nüß© Confusion Matrix (rows=true, cols=predicted)\")\n",
    "print(\" \" * 12 + \"  \".join([f\"{c:>10}\" for c in classes]))\n",
    "for i, cls in enumerate(classes):\n",
    "    row_str = \"  \".join([f\"{n:>10}\" for n in cm[i]])\n",
    "    print(f\"{cls:<12}{row_str}\")\n",
    "\n",
    "# Correct / Incorrect\n",
    "correct = int((y_true == y_pred).sum())\n",
    "total = int(len(y_true))\n",
    "incorrect = total - correct\n",
    "print(f\"\\n‚úÖ Correctly classified instances:   {correct} / {total}  ({correct/total*100:.2f}%)\")\n",
    "print(f\"‚ùå Incorrectly classified instances: {incorrect} / {total}  ({incorrect/total*100:.2f}%)\")\n",
    "\n",
    "# MAE / RMSE (probability-based) ‡πÅ‡∏•‡∏∞ RAE / RRSE (baseline = prior ‡∏Ç‡∏≠‡∏á‡∏ä‡∏∏‡∏î‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô)\n",
    "proba = xgb_pipe.predict_proba(X_eval)                 # [N, n_classes]\n",
    "y_onehot = np.eye(len(classes))[y_true]                # one-hot ‡∏Ç‡∏≠‡∏á‡∏õ‡πâ‡∏≤‡∏¢‡∏à‡∏£‡∏¥‡∏á\n",
    "\n",
    "abs_err = np.abs(y_onehot - proba).sum(axis=1) / 2.0\n",
    "mae = abs_err.mean()\n",
    "sq_err = ((y_onehot - proba) ** 2).sum(axis=1) / 2.0\n",
    "rmse = np.sqrt(sq_err.mean())\n",
    "print(f\"\\nüìê Mean absolute error (MAE): {mae:.6f}\")\n",
    "print(f\"üìê Root mean squared error (RMSE): {rmse:.6f}\")\n",
    "\n",
    "prior_counts = pd.Series(y_true).value_counts().reindex(range(len(classes)), fill_value=0).values\n",
    "prior_dist = prior_counts / prior_counts.sum()\n",
    "abs_err_base = np.abs(y_onehot - prior_dist).sum(axis=1) / 2.0\n",
    "mae_base = abs_err_base.mean()\n",
    "sq_err_base = ((y_onehot - prior_dist) ** 2).sum(axis=1) / 2.0\n",
    "rmse_base = np.sqrt(sq_err_base.mean())\n",
    "\n",
    "rae  = (mae / mae_base) * 100.0 if mae_base > 0 else float(\"inf\")\n",
    "rrse = (rmse / rmse_base) * 100.0 if rmse_base > 0 else float(\"inf\")\n",
    "print(f\"üìè Relative absolute error (RAE): {rae:.2f}%\")\n",
    "print(f\"üìè Root relative squared error (RRSE): {rrse:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
